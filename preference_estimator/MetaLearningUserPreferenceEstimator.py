# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iUzHvBOYqkKss1x5-Zg-zxs5cWWBUe_i
"""



import pandas as pd
import numpy as np
import torch
import random
import pandas as pd
from copy import deepcopy
from torch.utils.data import DataLoader, Dataset
from ast import literal_eval
import matplotlib.pyplot as plt
#Read the data
amazon_ratings = pd.read_csv(r"/content/drive/My Drive/amazon_reviews_us_Electronics_train.tsv",sep=",",header=0,error_bad_lines=False)
torch.manual_seed(2020)
#print(amazon_ratings)
user_id = amazon_ratings[['customer_id']].drop_duplicates().reindex()
user_id['userId'] = np.arange(len(user_id))
amazon_ratings = pd.merge(amazon_ratings, user_id, on=['customer_id'], how='left')
item_id = amazon_ratings[['product_id']].drop_duplicates()
item_id['itemId'] = np.arange(len(item_id))
amazon_ratings = pd.merge(amazon_ratings, item_id, on=['product_id'], how='left')
amazon_ratings = amazon_ratings[['userId', 'itemId', 'star_rating','aspect_rating']]
print('Range of userId is [{}, {}]'.format(amazon_ratings.userId.min(), amazon_ratings.userId.max()))
print('Range of itemId is [{}, {}]'.format(amazon_ratings.itemId.min(), amazon_ratings.itemId.max()))

l1 = []
for i in amazon_ratings["aspect_rating"]:
  i = literal_eval(i)
  l1.append(i)

amazon_ratings["aspect_rating"] = l1

from torch.utils.data import DataLoader, Dataset
from itertools import chain 

class UserItemRatingDataset(Dataset):
    """Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset"""
    def __init__(self, user_tensor, item_tensor, target_tensor,target_aspect_tensor):
        """
        args:
            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair
        """
        self.user_tensor = user_tensor
        self.item_tensor = item_tensor
        self.target_tensor = target_tensor
        self.target_aspect_tensor = target_aspect_tensor

    def __getitem__(self, index):
        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index] , self.target_aspect_tensor[index]

    def __len__(self):
        return self.user_tensor.size(0)


class SampleGenerator(object):
    """Construct dataset for NCF"""

    def __init__(self, ratings):
        """
        args:
            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']
        """
        assert 'userId' in ratings.columns
        assert 'itemId' in ratings.columns
        assert 'star_rating' in ratings.columns

        self.ratings = ratings
        # explicit feedback using _normalize and implicit using _binarize
        self.preprocess_ratings = self._normalize(ratings)
        #self.preprocess_ratings = self._binarize(ratings)
        self.user_pool = set(self.ratings['userId'].unique())
        self.item_pool = set(self.ratings['itemId'].unique())
        # create negative item samples for NCF learning
        #self.negatives = self._sample_negative(ratings)
        self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings)

    def _normalize(self, ratings):
        """normalize into [0, 1] from [0, max_rating], explicit feedback"""
        ratings = deepcopy(ratings)
        max_rating = ratings.star_rating.max()
        ratings['star_rating'] = ratings.star_rating * 1.0 / max_rating
        return ratings

    def _split_loo(self, ratings):
        """leave one out train/test split """
        ratings_1 = ratings.groupby(['userId'])
        size = ratings_1.size()
        customers = size[size > 5]
        keys = list(customers.keys())
        tr=[]
        te=[]
        for i in range(len(keys)):
          product_ids =list(ratings_1["itemId"].get_group(keys[i]))
          rate = list(ratings_1["star_rating"].get_group(keys[i]))
          aspect_rate = list(ratings_1["aspect_rating"].get_group(keys[i]))
          t = int(len(product_ids)*0.8)
          for j in range(t,len(product_ids)):
            te.append([keys[i],product_ids[j],rate[j],aspect_rate[j]])
          for j in range(0,t):
            tr.append([keys[i],product_ids[j],rate[j],aspect_rate[j]])
        train = pd.DataFrame(tr,columns=["userId","itemId","star_rating","aspect_rating"])
        test = pd.DataFrame(te,columns=["userId","itemId","star_rating","aspect_rating"])
        return train,test


    def _sample_negative(self, ratings):
        random.seed(10)
        """return all negative items & 100 sampled negative items"""
        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(
            columns={'itemId': 'interacted_items'})
        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)
        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 10))
        return interact_status[['userId', 'negative_items', 'negative_samples']]

    def instance_a_train_loader(self, num_negatives,negatives, batch_size):
        """instance train loader for one training epoch"""
        users , items, ratings,aspect_rating=[],[],[],[]
        train_ratings = pd.merge(self.train_ratings, negatives[['userId', 'negative_items']], on='userId')
        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))
        for row in train_ratings.itertuples():
            users.append(int(row.userId))
            items.append(int(row.itemId))
            ratings.append(float(row.star_rating))
            aspect_rating.append(list(row.aspect_rating))
            for i in range(num_negatives):
                users.append(int(row.userId))
                items.append(int(row.negatives[i]))
                ratings.append(float(0))  # negative samples get 0 rating
                aspect_rating.append([0,0,0,0])
        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),
                                        item_tensor=torch.LongTensor(items),
                                        target_tensor=torch.DoubleTensor(ratings),
                                        target_aspect_tensor=torch.FloatTensor(aspect_rating))
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)

    
    def evaluate_data(self,test_negative):
        """create evaluate data"""
        test_ratings = pd.merge(self.test_ratings, test_negative[['userId', 'negative_samples']], on='userId')
        
        test_users, test_items,ratings, test_aspect_ratings,negative_users, negative_items,negative_ratings, negative_aspect_ratings = [], [], [], [],[],[],[],[]
        for row in test_ratings.itertuples():
            test_users.append(int(row.userId))
            test_items.append(int(row.itemId))
            ratings.append(float(row.star_rating))
            test_aspect_ratings.append(row.aspect_rating)
            for i in range(len(row.negative_samples)):
                negative_users.append(int(row.userId))
                negative_items.append(int(row.negative_samples[i]))
                negative_ratings.append(0)
                negative_aspect_ratings.append([0,0,0,0])
        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.FloatTensor(ratings), torch.FloatTensor(test_aspect_ratings), torch.LongTensor(negative_users),
                torch.LongTensor(negative_items),torch.DoubleTensor(negative_ratings), torch.FloatTensor(negative_aspect_ratings)]

import torch
import numpy as np
from copy import deepcopy

from torch.autograd import Variable
from torch.nn import functional as F
from collections import OrderedDict

class EmbedU(torch.nn.Module):
  def __init__(self,input_dim,emb_dim):
    super(EmbedU, self).__init__()
    self.user_factor = torch.nn.Embedding(input_dim,emb_dim)

  def forward(self, user):
    return self.user_factor(user)

class EmbedI(torch.nn.Module):
  def __init__(self,input_dim,emb_dim):
    super(EmbedI, self).__init__()
    self.item_factor = torch.nn.Embedding(input_dim,emb_dim)

  def forward(self, item):
    return self.item_factor(item)

class user_preference_estimator(torch.nn.Module):
    def __init__(self, config):
        super(user_preference_estimator, self).__init__()
        self.embedding_dim = config['embedding_dim']
        self.fc1_in_dim = config['embedding_dim'] * 8
        self.fc2_in_dim = config['first_fc_hidden_dim']
        self.fc2_out_dim = config['second_fc_hidden_dim']
        self.use_cuda = config['use_cuda']
        self.user_e = EmbedU(84821,64)
        self.item_e = EmbedI(27811,64)
        
        self.fc1 = torch.nn.Linear(132,64)
        self.fc2 = torch.nn.Linear(64,1)
        self.linear_out = torch.nn.Linear(1,1)
     

    def forward(self, user,item,aspect):
        u = self.user_e(user)
        i = self.item_e(item)
        x = torch.cat([u,i])
        x = torch.cat([x,aspect])
        x = F.relu(x)
        x = self.fc1(x.T)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.linear_out(x)
        x = F.relu(x)
        return x.double()

class MeLU(torch.nn.Module):
    def __init__(self,config):
        super(MeLU, self).__init__()
        self.model = user_preference_estimator(config)
        self.local_lr = 5e-6
        self.store_parameters()
        self.meta_optim = torch.optim.Adam(self.model.parameters(), lr=5e-4)
        self.local_update_target_weight_name = ['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'linear_out.weight', 'linear_out.bias']

    def store_parameters(self):
        self.keep_weight = deepcopy(self.model.state_dict())
        self.weight_name = list(self.keep_weight.keys())
        self.weight_len = len(self.keep_weight)
        self.fast_weights = OrderedDict()

    def forward(self, user,item,rating, queryitem,support_aspect,query_aspect, num_local_update):
      for idx in range(num_local_update):
          if idx > 0:
              self.model.load_state_dict(self.fast_weights)
          weight_for_local_update = list(self.model.state_dict().values())
          support_set_y_pred = self.model(user,item,support_aspect)
          rating = rating.reshape(1)
          loss = F.mse_loss(support_set_y_pred,rating).double()
          self.model.zero_grad()
          grad = torch.autograd.grad(loss, self.model.parameters(), create_graph=True)
          # local update
          for i in range(self.weight_len):
              
              if self.weight_name[i] in self.local_update_target_weight_name:
                  self.fast_weights[self.weight_name[i]] = weight_for_local_update[i] - self.local_lr * grad[i]
              else:
                  self.fast_weights[self.weight_name[i]] = weight_for_local_update[i]
          
      
      self.model.load_state_dict(self.fast_weights)
      query_set_y_pred = self.model(user,queryitem,query_aspect)
      self.model.load_state_dict(self.keep_weight)
      return query_set_y_pred

    def global_update(self, support_set_xs_u, support_set_xs_i, support_set_ys, query_set_xs_u, query_set_xs_i, query_set_ys,support_aspect,query_aspect, num_local_update):
        batch_sz = len(support_set_xs_i)
        losses_q = []
        for i in range(batch_sz):
            query_set_y_pred = self.forward(support_set_xs_u[i],support_set_xs_i[i], support_set_ys[i], query_set_xs_i[i], support_aspect[i],query_aspect[i],num_local_update)
            query_set_ys[i] = query_set_ys[i].reshape(1)
            loss_q = F.mse_loss(query_set_y_pred,query_set_ys[i])
            losses_q.append(loss_q)
            #print(loss_q)
        losses_q = torch.stack(losses_q).mean(0)
        self.meta_optim.zero_grad()
        losses_q.backward()
        self.meta_optim.step()
        self.store_parameters()
        return losses_q

sample_generator = SampleGenerator(ratings=amazon_ratings)

train_negatives = sample_generator._sample_negative(sample_generator.train_ratings)
test_negatives = sample_generator._sample_negative(sample_generator.test_ratings)

evaluate_data = sample_generator.evaluate_data(test_negatives)

def training(melu,user,item,rating,aspect_rating):
  
  supp_x_u = []
  supp_x_i = []
  query_x_i = []
  supp_x_r = []
  query_x_r = []
  query_x_u = []
  supp_aspect = []
  query_aspect = []
  t = int(0.3*len(item))
  for i in range(t):
    supp_x_i.append(item[i])
    supp_x_r.append(rating[i])
    supp_x_u.append(user[i])
    supp_aspect.append(aspect_rating[i])
  for i in range(t,len(item)):
    query_x_i.append(item[i])
    query_x_r.append(rating[i])
    query_x_u.append(user[i])
    query_aspect.append(aspect_rating[i])
  return melu.global_update(user,supp_x_i,supp_x_r,user,query_x_i,query_x_r,supp_aspect,query_aspect,1)

config = {
    # item
    'num_rate': 6,
    'num_genre': 25,
    'num_director': 2186,
    'num_actor': 8030,
    'embedding_dim': 32,
    'first_fc_hidden_dim': 64,
    'second_fc_hidden_dim': 1,
    # user
    'num_gender': 2,
    'num_age': 7,
    'num_occupation': 21,
    'num_zipcode': 3402,
    # cuda setting
    'use_cuda': False,
    # model setting
    'inner': 1,
    'lr': 5e-5,
    'local_lr': 5e-6,
    'batch_size': 32,
    'num_epoch': 20,
    # candidate selection
    'num_candidate': 20,
}

melu = MeLU(config)
epochs=[]
mae=[]
for epoch_number in range(1,11):
  plt.figure()
  epochs.append(epoch_number)
  losses=[]
  ids=[]
  s=0
  train_loader = sample_generator.instance_a_train_loader(6,train_negatives,512)
  for batch_id,batch in enumerate(train_loader):
    user = batch[0]
    item = batch[1]
    ratings = batch[2]
    aspect_ratings = batch[3]
    losses.append(training(melu,user,item,ratings,aspect_ratings))
    ids.append(batch_id+1)
  plt.plot(ids,losses)
  print("------------------------------------------------------------------------------------------------------")
  test_users, test_items , ratings, test_aspect_ratings = evaluate_data[0], evaluate_data[1] , evaluate_data[2], evaluate_data[3]
  for i in range(len(test_users)):
    pred_y = melu.model(test_users[i],test_items[i],test_aspect_ratings[i])
    rates = ratings[i].reshape(1)
    s+=F.l1_loss(pred_y,rates)
  mae.append(s/len(test_users))
plt.figure()
plt.plot(epochs,mae)

print(len(test_users))
print(len(test_items))
print(len(test_aspect_ratings))
print(len(ratings))

print(losses)

print(mae)

