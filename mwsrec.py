# -*- coding: utf-8 -*-
"""MWSRec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bpsrZcVWkhNbP4istF0gPMuR4vlQdM9p
"""

# Imports
import pandas as pd
import numpy as np
import torch
import random

from copy import deepcopy
from ast import literal_eval
import matplotlib.pyplot as plt

from torch.utils.data import DataLoader, Dataset
from itertools import chain 

#handling text data
from torchtext import data as dt
from tqdm import tqdm

from ast import literal_eval
import matplotlib.pyplot as plt

import gensim
from gensim.models import KeyedVectors

import re
import spacy

import torch.nn as nn
import numpy as np
from copy import deepcopy
 
from torch.autograd import Variable
from torch.nn import functional as F
from collections import OrderedDict
import en_core_web_sm

import math

import datetime
import time


# Data file loading ------------------------------------------------------------

# Set the paths
data_path = "/home/paras_tharindu/yelp_train_v1.tsv"
save_path = "/home/paras_tharindu/Model_Files/"
plot_path = "/home/paras_tharindu/Plots/"

#Read the data
amazon_ratings = pd.read_csv(data_path,sep=",",header=0,error_bad_lines=False)

torch.manual_seed(2020)
user_id = amazon_ratings[['customer_id']].drop_duplicates().reindex()
user_id['userId'] = np.arange(len(user_id))
amazon_ratings = pd.merge(amazon_ratings, user_id, on=['customer_id'], how='left')
item_id = amazon_ratings[['product_id']].drop_duplicates()
item_id['itemId'] = np.arange(len(item_id))
amazon_ratings = pd.merge(amazon_ratings, item_id, on=['product_id'], how='left')
amazon_ratings = amazon_ratings[['userId', 'itemId', 'review_body','star_rating']]

print('Range of userId is [{}, {}]'.format(amazon_ratings.userId.min(), amazon_ratings.userId.max()))
print('Range of itemId is [{}, {}]'.format(amazon_ratings.itemId.min(), amazon_ratings.itemId.max()))

# ------------------------------------------------------------------------------

# Sample Generator class for training data creation ----------------------------


class UserItemRatingDataset(Dataset):
    """Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset"""
    def __init__(self, user_tensor, item_tensor, review_clf_tensor, target_tensor):
        """
        args:
            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair
        """
        self.user_tensor = user_tensor
        self.item_tensor = item_tensor
        self.review_clf_tensor = review_clf_tensor
        self.target_tensor = target_tensor

    def __getitem__(self, index):
      get_output = [self.user_tensor[index], self.item_tensor[index], self.review_clf_tensor[index], self.target_tensor[index]]

      return get_output

    def __len__(self):
        return self.user_tensor.size(0)


class SampleGenerator(object):
    """Construct dataset for NCF"""

    def __init__(self, ratings, config):
        """
        args:
            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']
        """
        assert 'userId' in ratings.columns
        assert 'itemId' in ratings.columns
        assert 'review_body' in ratings.columns
        assert 'star_rating' in ratings.columns

        self.NLP = en_core_web_sm.load()

        self.MAX_CHARS = config['MAX_CHARS']

        self.ratings = ratings

        self.vocab, self.emb_mat = self._process_reviews(ratings)

        # explicit feedback using _normalize and implicit using _binarize
        # self.preprocess_ratings = self._normalize(ratings)
        self.preprocess_ratings = ratings
        #self.preprocess_ratings = self._binarize(ratings)
        self.user_pool = set(self.ratings['userId'].unique())
        self.item_pool = set(self.ratings['itemId'].unique())
        self.train_ratings, self.test_ratings, self.user_map, self.item_map = self._split_loo(self.preprocess_ratings, config)

    def _tokenizer(self,review):

        review = re.sub(
            r"[\*\"“”\n\\…\+\-\/\=\(\)‘•:\[\]\|’\!;]", " ", 
            str(review))
        review = re.sub(r"[ ]+", " ", review)
        review = re.sub(r"\!+", "!", review)
        review = re.sub(r"\,+", ",", review)
        review = re.sub(r"\?+", "?", review)
        if (len(review) > self.MAX_CHARS):
            review = review[:self.MAX_CHARS]
        return [x.text for x in self.NLP.tokenizer(review) if x.text != " "]

    def _process_reviews(self,ratings):

      REVIEW = dt.Field(
      sequential=True,
      fix_length=config['max_sentence_len'],
      tokenize=self._tokenizer,
      pad_first=True,
      dtype=torch.cuda.LongTensor,
      lower=True )

      review_data = []

      for element in tqdm(ratings['review_body'].values):
        review_data.append(self._tokenizer(element))

      REVIEW.build_vocab(
              review_data,
              max_size=config['max_vocab'],
              min_freq=1,
              vectors=None
          )

      word_vocab = REVIEW.vocab.stoi

      # Maximum number of unique words in the training vocublary
      config['nb_words'] = min(config['max_vocab'], len(word_vocab))

      # Create a random embedding matrix for each word in the training set 
      embedding_matrix = np.random.normal(config['emb_mean'], config['emb_std'], (config['nb_words']+1, config['word_emb_size']))

      # Loading google word2vec embeddings 
      word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)

      # Populate random embedding matrix with google word2vec embeddings 
      i = 0
      for word in word_vocab:
          i +=1
          if i >= config['nb_words']: continue

          try:
              embedding_vector = word2vec[word]
          except:
              embedding_vector = None
          if embedding_vector is not None: embedding_matrix[i] = embedding_vector

      return word_vocab, embedding_matrix


    def _normalize(self, ratings):
        """normalize into [0, 1] from [0, max_rating], explicit feedback"""
        ratings = deepcopy(ratings)
        max_rating = ratings.star_rating.max()
        ratings['star_rating'] = ratings.star_rating * 1.0 / max_rating
        return ratings
    
    
    def _split_loo(self, ratings, config):
        """leave one out train/test split """
        
        ratings_1 = ratings.groupby(['userId'])
        size = ratings_1.size()
        customers = size[size > config['min_rating_count']]
        keys = list(customers.keys())
        unique_products = list(ratings[ratings['userId'].isin(keys)]['itemId'].unique())

        tr=[]
        te=[]
        for i in tqdm(range(len(keys))):
          product_ids =list(ratings_1["itemId"].get_group(keys[i]))
          reviews = list(ratings_1["review_body"].get_group(keys[i]))
          rate = list(ratings_1["star_rating"].get_group(keys[i]))

          t = int(len(product_ids)*config['split_ratio'])
          for j in range(t,len(product_ids)):
            row = [i,unique_products.index(product_ids[j]),reviews[j],rate[j]]

            te.append(row)

          for j in range(0,t):
            row = [i,unique_products.index(product_ids[j]),reviews[j],rate[j]]

            tr.append(row)
        
        col_names = ["userId","itemId","review_body","star_rating"]


        train = pd.DataFrame(tr,columns=col_names)
        test = pd.DataFrame(te,columns=col_names)
        return train,test, keys, unique_products


    def _sample_negative(self, ratings):
        random.seed(10)
        """return all negative items & 100 sampled negative items"""
        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(
            columns={'itemId': 'interacted_items'})
        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: set(list(range(len(self.item_map)))) - x)
        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 10))
        return interact_status[['userId', 'negative_items', 'negative_samples']]
  
    def instance_a_train_loader(self, batch_size, pad_token= '<pad>'):
        """instance train loader for one training epoch"""
        users,items,ratings,review_cls=[],[],[],[]
        train_ratings = self.train_ratings

        for row in train_ratings.itertuples():
            users.append(int(row.userId))
            items.append(int(row.itemId))
            ratings.append(float(row.star_rating))

            sentence_words = self._tokenizer(row.review_body)

            if len(sentence_words) >= config['max_sentence_len']:
              sentence_words = sentence_words[0:config['max_sentence_len']]

            word_idx = []

            for tok in sentence_words:
              if self.vocab[tok] > config['nb_words']:
                word_idx.append(0)
              else:
                word_idx.append(self.vocab[tok])

            # word_idx = [self.vocab[tok] for tok in sentence_words]

            if len(word_idx) >= config['max_sentence_len']:
              word_idx = word_idx[0:config['max_sentence_len']]

            else:
              word_idx += [self.vocab[pad_token]] * (config['max_sentence_len'] - len(word_idx))

            review_cls.append(word_idx)
            
               
        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),
                                        item_tensor=torch.LongTensor(items),
                                        review_clf_tensor=torch.LongTensor(review_cls),
                                        target_tensor=torch.DoubleTensor(ratings)
                                        )
        
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)


    def evaluate_data(self):
        """create evaluate data"""
        test_ratings = self.test_ratings
        test_users,test_items,ratings =[],[],[]
       
        for row in test_ratings.itertuples():
            test_users.append(int(row.userId))
            test_items.append(int(row.itemId))
            ratings.append(float(row.star_rating))
                
        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.DoubleTensor(ratings)]

# ------------------------------------------------------------------------------

# MWSRec Model classes 

import math

class EmbedU(torch.nn.Module):
  def __init__(self,input_dim,emb_dim):
    super(EmbedU, self).__init__()
    self.user_factor = torch.nn.Embedding(input_dim,emb_dim)
 
  def forward(self, user):
    user = user.cuda()
    return self.user_factor(user)
 
class EmbedI(torch.nn.Module):
  def __init__(self,input_dim,emb_dim):
    super(EmbedI, self).__init__()
    self.item_factor = torch.nn.Embedding(input_dim,emb_dim)
 
  def forward(self, item):
    item = item.cuda()
    return self.item_factor(item)
 
class Attention(torch.nn.Module):
  def __init__(self, num_aspects, hidden_size, method="general"):
    super(Attention, self).__init__()
    self.method = method
    self.num_aspects = num_aspects
    self.hidden_size = hidden_size
   
    # Defining the layers/weights required depending on alignment scoring method
    if method == "general":
      self.fc1 = nn.Linear(hidden_size*(num_aspects+1), num_aspects+1, bias=False)
     
 
  def forward(self, inputs):
    inputs = inputs.cuda()
    if self.method == "general":
 
      return F.softmax(self.fc1(inputs))


class EmbedS(torch.nn.Module):
  def __init__(self,input_dim,emb_dim, init_weight_mat = None, method='conv'):
    super(EmbedS, self).__init__()
    self.review_factor = torch.nn.Embedding(input_dim,emb_dim)

    if init_weight_mat is not None:
      self.review_factor.weight.data = init_weight_mat

    self.method = method
 
  def forward(self, sentence):
    #item = item.cuda()
    if self.method == 'sum':
      return torch.sum(self.review_factor(sentence), dim=0)
    if self.method == 'conv':
      return self.review_factor(sentence)


class Teacher(nn.Module):
    def __init__(self, config, emb_mat=None):
        super().__init__()
        self.embedding_dim = config['embedding_dim']

        self.num_aspects = config['num_aspects']
        self.num_users = config['num_users']
        self.num_items = config['num_items']
        self.num_words = config['nb_words']
        self.word_emb_size = config['word_emb_size']

        self.input_chnl = config['input_chnl']
        self.num_kernals = config['num_kernals']
        self.kernel_sizes = config['kernel_sizes']
        self.dropout = config['dropout']

        self.fc1_in_dim = config['word_emb_size']
        self.fc2_in_dim = config['t_first_fc_hidden_dim']
        self.fc2_out_dim = config['t_second_fc_hidden_dim']

        # Symptom embeddings
        self.review_emb = EmbedS(self.num_words+1,self.word_emb_size, init_weight_mat=emb_mat)

        self.convs = torch.nn.ModuleList([nn.Conv2d(self.input_chnl, self.num_kernals, (K, self.word_emb_size)) for K in self.kernel_sizes])
        self.dropout = torch.nn.Dropout(self.dropout)
        self.t_fc1 = torch.nn.Linear(len(self.kernel_sizes) * self.num_kernals, self.fc2_in_dim)

        # self.fc1 = torch.nn.Linear(self.fc1_in_dim,self.fc2_in_dim)
       
        self.t_fc2 = torch.nn.Linear(self.fc2_in_dim,self.num_aspects)

        # self.init_weights()

    # def init_weights(self):
    #     initrange = 0.5
    #     self.embedding.weight.data.uniform_(-initrange, initrange)
    #     self.fc.weight.data.uniform_(-initrange, initrange)
    #     self.fc.bias.data.zero_()

    def forward(self, review):

      review = review.cuda()
  
      re = self.review_emb(review).cuda()

      x = re.reshape([1, 1]+list(re.size()))  # (N, Ci, W, D)

      x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(N, Co, W), ...]*len(Ks)

      x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)

      x = torch.cat(x, 1)

      x = self.dropout(x)  # (N, len(Ks)*Co)
      x = self.t_fc1(x)  # (N, C)
        
      x = x.squeeze()
      # x = self.review_emb(review)
      # x = self.fc1(x)
      x = F.relu(x)
      x = self.t_fc2(x)
      x = F.relu(x)
      output_aspects = x.double()
 
      return output_aspects

 
class user_preference_estimator(torch.nn.Module):
    def __init__(self, config):
        super(user_preference_estimator, self).__init__()
        self.embedding_dim = config['embedding_dim']
        self.fc1_in_dim = config['embedding_dim'] * 2
        self.fc2_in_dim = config['first_fc_hidden_dim']
        self.fc2_out_dim = config['second_fc_hidden_dim']
        self.linear_out = config['linear_layer']
        self.use_cuda = config['use_cuda']
        self.num_aspects = config['num_aspects']
        self.num_users = config['num_users']
        self.num_items = config['num_items']
 
        # User embeddings for each aspect
        self.user_emb = torch.nn.ModuleList()
        for j in range(config['num_aspects']+1):
          self.user_emb.append(EmbedU(self.num_users,self.embedding_dim).cuda())

        # item embeddings for each aspect
        self.item_emb = torch.nn.ModuleList()

        for j in range(config['num_aspects']+1):
          self.item_emb.append(EmbedI(self.num_items,self.embedding_dim).cuda())
    
        # User attention layers
        self.user_attentions = torch.nn.ModuleList()
        for j in range(config['num_aspects']+1):
          self.user_attentions.append(Attention(self.num_aspects, self.embedding_dim).cuda())

        # Item attention layers
        self.item_attentions = torch.nn.ModuleList()
        for j in range(config['num_aspects']+1):
          self.item_attentions.append(Attention(self.num_aspects, self.embedding_dim).cuda())

        self.fc1 = torch.nn.ModuleList()
        for j in range(config['num_aspects']+1):
          self.fc1.append(torch.nn.Linear(self.fc1_in_dim,self.fc2_in_dim))

        
        self.fc2 = torch.nn.ModuleList()
        for j in range(config['num_aspects']+1):
          self.fc2.append(torch.nn.Linear(self.fc2_in_dim,self.fc2_out_dim))

        self.linear_layer = torch.nn.ModuleList()
        for j in range(config['num_aspects']+1):
          self.linear_layer.append(torch.nn.Linear(self.fc2_out_dim,1))

    def forward(self, user,item):
        user = user.cuda()
        item = item.cuda()
        users = []
        items = []
 
        for id in range(self.num_aspects+1):
          users.append(self.user_emb[id](user).cuda())
          items.append(self.item_emb[id](item).cuda())
        
        u_input = torch.cat(users)
        i_input = torch.cat(items)
        
        u_input = u_input.cuda()
        i_input = i_input.cuda()
 
        user_attention_weights = []
        item_attention_weights = []
       
        for id in range(self.num_aspects+1):
          user_attention_weights.append(self.user_attentions[id](u_input).cuda())
          item_attention_weights.append(self.item_attentions[id](i_input).cuda())

        
        u = []
        i = []
 
        for id in range(self.num_aspects+1):
          u.append(torch.empty(self.embedding_dim, ).cuda())
          i.append(torch.empty(self.embedding_dim, ).cuda())
 
          u[id][u[id]!=0] = 0
          i[id][i[id]!=0] = 0
      
 
        for idx in range(self.num_aspects+1):
          for idy in range(self.num_aspects+1):
            u[idx] += users[idx]*user_attention_weights[idx][idy]
            i[idx] += items[idx]*item_attention_weights[idx][idy]
 
        output_loss = []
        for idx in range(self.num_aspects+1):
          x = torch.cat([u[idx],i[idx]])
          x = self.fc1[idx](x)
          x = F.relu(x)
          x = self.fc2[idx](x)
          x = F.relu(x)
          # x = self.linear_layer[idx](x)
          # x = F.relu(x)
          output_loss.append(x.double())
 
        return output_loss
 
class MWSRec(torch.nn.Module):
    def __init__(self,config, init_w):
        super(MWSRec, self).__init__()
        self.teacher, self.teacher_optim = self.init_teacher(config, init_w)
        self.model = user_preference_estimator(config)
        self.local_lr = config['local_lr']
        self.store_parameters()
        self.meta_optim = torch.optim.Adam(self.model.parameters(), lr=config['lr'])
        self.local_update_target_weight_name = ['fc1.0.weight','fc1.0.bias','fc1.1.weight','fc1.1.bias','fc1.2.weight','fc1.2.bias',
                                                'fc1.3.weight','fc1.3.bias','fc1.4.weight','fc1.4.bias','fc2.0.weight','fc2.0.bias',
                                                'fc2.1.weight','fc2.1.bias','fc2.2.weight','fc2.2.bias','fc2.3.weight','fc2.3.bias',
                                                'fc2.4.weight','fc2.4.bias','linear_out.0.weight','linear_out.0.bias','linear_out.1.weight',
                                                'linear_out.1.bias','linear_out.2.weight','linear_out.2.bias','linear_out.3.weight',
                                                'linear_out.3.bias','linear_out.4.weight','linear_out.4.bias']
 
    def init_teacher(self, config, init_w):
      teacher_model = Teacher(config=config, emb_mat=init_w)

      teacher_model.cuda()

      step_size = config['t_step_size']

      teacher_optimizer = torch.optim.Adam(teacher_model.parameters(), 
                                lr=config['t_lr'])
      
      return teacher_model, teacher_optimizer

    def store_parameters(self):
        self.keep_weight = deepcopy(self.model.state_dict())
        self.weight_name = list(self.keep_weight.keys())
        self.weight_len = len(self.keep_weight)
        self.fast_weights = OrderedDict()
 
    def forward(self, user,item,rating, queryitem,support_aspects,num_local_update):
      for idx in range(num_local_update):
          if idx > 0:
              self.model.load_state_dict(self.fast_weights)

          weight_for_local_update = list(self.model.state_dict().values())

          predictions = self.model(user,item)
          
          rating = rating.reshape(1)

          for j in range(config['num_aspects']):
            support_aspects[j] = support_aspects[j].reshape(1)

          overall_loss = F.mse_loss(predictions[0],rating)

          loss = config['self_w']*overall_loss.double()

          for j in range(config['num_aspects']):
            support_aspects[j] = support_aspects[j].reshape(1)
            loss += F.mse_loss(predictions[1+j],support_aspects[j]).double()

          # loss = config['self_w']*overall_loss + config['weak_w']*loss

          self.model.zero_grad()
          grad = torch.autograd.grad(loss, self.model.parameters(), create_graph=True,allow_unused=True)
          # local update
          for i in range(self.weight_len):
             
              if self.weight_name[i] in self.local_update_target_weight_name:
                  self.fast_weights[self.weight_name[i]] = weight_for_local_update[i] - self.local_lr * grad[i]
              else:
                  self.fast_weights[self.weight_name[i]] = weight_for_local_update[i]
         
     
      self.model.load_state_dict(self.fast_weights)
      query_predictions = self.model(user,queryitem)
      self.model.load_state_dict(self.keep_weight)

      return query_predictions
 
    def global_update(self, support_set_xs_u, support_set_xs_i, support_set_ys, query_set_xs_u, 
                      query_set_xs_i, query_set_ys,num_local_update, review_cls):
        batch_sz = len(support_set_xs_i)
        losses_q = []
        teacher_batch_loss = []


        for i in range(batch_sz):
            support_set_xs_u[i] = support_set_xs_u[i].cuda()
            support_set_xs_i[i] = support_set_xs_i[i].cuda()
            support_set_ys[i] = support_set_ys[i].cuda()
            query_set_xs_u[i] = query_set_xs_u[i].cuda()
            query_set_xs_i[i] = query_set_xs_i[i].cuda()
            query_set_ys[i] = query_set_ys[i].cuda()
            review_cls[i] = review_cls[i].cuda()

            
        for i in range(batch_sz):

            teacher_output = self.teacher(review_cls[i])

            student_target = teacher_output.tolist()
            student_target = torch.FloatTensor(student_target).double()
            student_target = student_target.cuda()

            predictions = self.forward(support_set_xs_u[i],support_set_xs_i[i], 
                                       support_set_ys[i], query_set_xs_i[i],
                                       teacher_output, 
                                       num_local_update)
            
            # print(predictions)
            teacher_target = [torch.FloatTensor([element.item()]) for element in predictions]
            teacher_target = torch.FloatTensor(teacher_target).double()
            teacher_target = teacher_target.cuda()

            teacher_loss = 0

            for j in range(config['num_aspects']):
              teacher_loss += F.mse_loss(teacher_output[j], teacher_target[j])
            
            query_set_ys[i] = query_set_ys[i].reshape(1)

            overall_loss = F.mse_loss(predictions[0],query_set_ys[i])
            loss_q = config['self_w']*overall_loss

            for j in range(config['num_aspects']):
              loss_q += F.mse_loss(predictions[1+j], student_target[j])

            # loss_q = config['self_w']*overall_loss + config['weak_w']*loss_q

            teacher_batch_loss.append(teacher_loss)
            losses_q.append(loss_q)

        teacher_batch_loss = torch.stack(teacher_batch_loss).mean(0)
        self.teacher_optim.zero_grad()
        teacher_batch_loss.backward()
        self.teacher_optim.step()

        losses_q = torch.stack(losses_q).mean(0)
        self.meta_optim.zero_grad()
        losses_q.backward()
        self.meta_optim.step()
        self.store_parameters()
        return losses_q, teacher_batch_loss
 
    def evaluation(self,support_set_x_u, support_set_x_i, support_set_y, num_local_update):
      all_losses=[]
      # total_losses = []
      for i in range(len(support_set_x_u)):
        loss = []
        # support_set_y_pred, aspect1_ratings_pred,aspect2_ratings_pred,aspect3_ratings_pred,aspect4_ratings_pred = 
        predictions = self.model(support_set_x_u[i], support_set_x_i[i])
        actual_rating = support_set_y[i].reshape(1)

        loss.append(F.mse_loss(predictions[0], actual_rating).item())

        all_losses.append(loss)

      return all_losses

# ------------------------------------------------------------------------------

# Model Training function ------------------------------------------------------

def training(mwsrec,user,item,review,rating):
  supp_x_u = []
  supp_x_i = []
  query_x_i = []
  supp_x_r = []
  query_x_r = []
  query_x_u = []


  mwsrec.cuda()
  t = int(0.3*len(item))
  for i in range(t):
    supp_x_i.append(item[i])
    supp_x_r.append(rating[i])
    supp_x_u.append(user[i])
 
  for i in range(t,len(item)):
    query_x_i.append(item[i])
    query_x_r.append(rating[i])
    query_x_u.append(user[i])

  return mwsrec.global_update(user,supp_x_i,supp_x_r,user,query_x_i,query_x_r, 
                            config['num_local_update'], review)


# ------------------------------------------------------------------------------

# Configurations ---------------------------------------------------------------

config = {
    # item
    'num_users': 84821,
    # item
    'num_items': 27811,
    
    'min_rating_count':2,
    'split_ratio':0.8, 

    'word_emb_size': 300,
    'max_vocab': 5000,
    'MAX_CHARS':20000,
    'emb_mean': 0.004451992,
    'emb_std': 0.4081574,
    'max_sentence_len': 50,
    'nb_words':0,

    # Student network
    'embedding_dim': 8,
    'first_fc_hidden_dim': 32,
    'second_fc_hidden_dim': 1,
    'linear_layer':1,
    'num_aspects':4,

    #Tecaher network
    't_first_fc_hidden_dim': 50,
    't_second_fc_hidden_dim': 8,
    't_step_size':300,
    't_lr':1e-7,
    'input_chnl':1,
    'num_kernals': 100,
    'kernel_sizes': [3, 4, 5],
    'dropout': 0.5,

    #Network weights
    'self_w': 0.1,
    'weak_w': 0.3,

    # cuda setting
    'use_cuda': True,

    # model setting
    'inner': 1,
    'lr': 1e-5,
    'local_lr': 1e-7,
    'num_local_update':1,
    'batch_size': 32,
    'num_epoch': 200,
    'num_negatives': 0,
}

# ------------------------------------------------------------------------------

# Pre-trained W2VEC embedding 
!wget -P /home/paras_tharindu/ -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
EMBEDDING_FILE = '/home/paras_tharindu/GoogleNews-vectors-negative300.bin.gz'


# Creating the training data sample generator object 
sample_generator = SampleGenerator(ratings=amazon_ratings, config=config)

# Training starts here 

config['num_users'] = len(sample_generator.user_map)
config['num_items'] = len(sample_generator.item_map)

print()
print("-------- Training Data Report -----------")

print("No. min rating per user:", config['min_rating_count'])
print("Size of the user item matrix ", config['num_users'], "x", config['num_items'])
print("Mean number of items per given user: ", 
      sample_generator.train_ratings[["userId", "itemId"]].groupby(["userId"]).count().mean())

print()
print("-------- ------------------- -----------")

epochs=[]
train_losses=[]
val_losses = []

weight_mat = torch.FloatTensor(sample_generator.emb_mat)

mwsrec = MWSRec(config, init_w=weight_mat)

ts = time.time()
st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
print()

model_name = 'Unno_Tronic_One_Stage'+str(st)+"_"+str(config['min_rating_count'])+'_K'+str(config['num_aspects'])+'_Em'+str(config['embedding_dim'])+'_E'+str(config['num_epoch'])+'_B'+str(config['batch_size'])
print(model_name)
print("Validation and Testing data splitting :")
evaluate_data = sample_generator.evaluate_data()
val_test_ratio = 0.2

validate_data = []
test_data = []
l=0
for data_tensor in evaluate_data[0:3]:
  l = len(data_tensor)
  split_size = int(len(data_tensor)*val_test_ratio)
  split_sizes = [split_size, len(data_tensor)-split_size]
  validate_data_tensor, test_data_tensor = torch.split(data_tensor, split_size_or_sections=split_sizes)
  validate_data.append(validate_data_tensor)
  test_data.append(test_data_tensor)

print(len(validate_data[0]),len(validate_data[1]),len(validate_data[2]))
print(len(test_data[0]),len(test_data[1]),len(test_data[2]))

# -----------------------------------------------------------
# Get validate users and items 
validate_users, validate_items , validate_ratings = validate_data[0].cuda(), validate_data[1].cuda() , validate_data[2].cuda()

# print("-------------- Splitting End ----------------:")
print()
# Early stopping variables
min_val_loss = 1000
epochs_no_improve = 0
val_loss = 0
n_epochs_stop = 10 # Stopping window : Check for 10 epochs whether the validation loss improves, else stop the model training

print("Training started :", model_name)

for epoch_number in range(0,config['num_epoch']):
  total_loss=0
  tot_teacher_loss = 0
  c=0
  epochs.append(epoch_number)
  train_loader = sample_generator.instance_a_train_loader(batch_size=config['batch_size']) 

  for batch_id,batch in enumerate(train_loader):
    c+=1
    users = batch[0]
    items = batch[1]
    review_cls = batch[2].cuda()
    ratings = batch[3]

    loss,teacher_loss = (training(mwsrec,users,items,review_cls,ratings))
    total_loss+= loss.item()
    tot_teacher_loss +=teacher_loss.item()
  
  val_loss = mwsrec.evaluation(validate_users, validate_items, validate_ratings, config['num_local_update'])
  
  val_loss_li = []
  for aspect_id in range(1):
    aspect_val = [val_loss[i][aspect_id] for i in range(len(val_loss))]
    val_loss_li.append(sum(aspect_val) / float(len(aspect_val)))

  val_loss = sum(val_loss_li)
  
  # If the validation loss is at a minimum
  if val_loss < min_val_loss:
        # Save the model
        checkpoint_model = deepcopy(mwsrec)

        epochs_no_improve = 0
        min_val_loss = val_loss
 
  else:
      epochs_no_improve += 1
      # Check early stopping condition
      if epochs_no_improve == n_epochs_stop:
          print('Early stopping!' )
          break
 
  train_loss = total_loss/c
  teacher_train_loss = tot_teacher_loss/c
  # print("Epoch ",epoch_number, " Training error (mse) ---", train_loss, " Validation error (mse) ---", val_loss)
  
  print("Epoch ",epoch_number, " Training error (mse) ---", train_loss, " Teacher training error (mse) ---", teacher_train_loss, " Validation error (mse) ---", val_loss)
  
  if (epoch_number+1)%10 == 0 :
    checkpoint = {'model': checkpoint_model,
          'state_dict': checkpoint_model.state_dict()
          }

    torch.save(checkpoint,save_path+model_name+"_checkpoint_"+str(epoch_number))

  train_losses.append(train_loss)
  val_losses.append(val_loss_li)

checkpoint = {'model': checkpoint_model,
          'state_dict': checkpoint_model.state_dict()
          }

torch.save(checkpoint,save_path+model_name)

print("------- Training Ended ", model_name, " ----------")
print()

# ------------------------------------------------------------------------------

# Model evaluation -------------------------------------------------------------

print("------- Model evaluation Started ", model_name, " ----------")
evaluate_data = sample_generator.evaluate_data()
val_test_ratio = 0.2

validate_data = []
test_data = []
l=0
for data_tensor in evaluate_data[0:3]:
  l = len(data_tensor)
  split_size = int(len(data_tensor)*val_test_ratio)
  split_sizes = [split_size, len(data_tensor)-split_size]
  validate_data_tensor, test_data_tensor = torch.split(data_tensor, split_size_or_sections=split_sizes)
  validate_data.append(validate_data_tensor)
  test_data.append(test_data_tensor)

test_users, test_items, test_ratings = test_data[0].cuda(), test_data[1].cuda() , test_data[2].cuda()
test_scores = []
negative_scores = []
loss = 0
loss2 = 0
c = 0
mae_list = []
rmse_list = []

print(type(test_ratings))
for i in range(len(test_users)):
  l = checkpoint_model.model(test_users[i],test_items[i])
  mae = abs(l[0] - test_ratings[i])
  rmse = (abs(l[0] - test_ratings[i]))**2
  loss += mae
  loss2+= rmse
  c+=1
  test_scores.append(l[0])
  mae_list.append(mae.item())
  rmse_list.append(rmse.item())
mae = loss/c
rmse = (loss2/c)**0.5


print("MAE Value is: ",mae, " -- std:",np.std(mae_list))
print("RMSE Value is: ",rmse, " -- std:",((np.std(rmse_list))**0.5)/(len(rmse_list)**0.5))